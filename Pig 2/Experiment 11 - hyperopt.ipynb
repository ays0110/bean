{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import dicom\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import scipy.misc\n",
    "from skimage import io\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import skimage.color\n",
    "from skimage.filters import threshold_otsu\n",
    "import keras\n",
    "\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe3f391c5f8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAFNCAYAAAC5YlyiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWqtJ8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe4128d07b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "realigned_atlas = nib.load('../Data/big_atlas/coregistered_pig_2_atlas.nii').get_data()\n",
    "#original_atlas = nib.load('../../Data/big_atlas/atlas.nii').get_data()\n",
    "anatomical = nib.load('../Data/pig_2/coregistered_anatomy.nii').get_data()\n",
    "mean_fmri = nib.load('../Data/pig_2/rfmri nii/meanus822-0016-00001-000001-01.nii').get_data()  \n",
    "\n",
    "mask_roi_1 = np.ma.masked_where(np.isin(realigned_atlas, [0]), realigned_atlas)\n",
    "fmri_masked_roi = np.ma.masked_where(np.ma.getmask(mask_roi_1), mean_fmri) \n",
    "# print(fmri_masked_roi)\n",
    "plt.imshow(fmri_masked_roi[:,:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee3d8739d3f4c438710a54bf1168ad0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/pig_2/rfmri nii/meanus822-0016-00001-000001-01.nii\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's try loading the entire thing to ram\n",
    "# the big tensor will have dimension [time, height, width, depth]\n",
    "\n",
    "im = mean_fmri\n",
    "all_data = np.zeros((300, im.shape[0], im.shape[1], im.shape[2]))\n",
    "masked_data = np.ma.zeros((300, im.shape[0], im.shape[1], im.shape[2]))\n",
    "\n",
    "dicom_files = [x for x in Path('../Data/pig_2/rfmri nii/').iterdir() if x.is_file() and not 'ds_store' in str(x).lower()]\n",
    "dicom_files.sort(key=str)\n",
    "time_index = 0\n",
    "mask_roi_1 = np.ma.masked_where(np.isin(realigned_atlas, [0]), realigned_atlas)\n",
    "\n",
    "for f in tqdm_notebook(dicom_files):\n",
    "    time_file = nib.load(str(f)).get_data()\n",
    "    fmri_masked_roi = np.ma.masked_where(np.ma.getmask(mask_roi_1), time_file) \n",
    "    try:\n",
    "        masked_data[time_index, ...] = fmri_masked_roi\n",
    "        all_data[time_index, ...] = time_file\n",
    "        time_index += 1\n",
    "    except:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert all_data.shape == (300, 128, 128, 20) #300 time samples, 128x128 image, over 20 slices in z-index\n",
    "assert masked_data.shape == (300, 128, 128, 20) # basic sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe3f08150f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAFNCAYAAAC5YlyiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe40c439390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(masked_data[1,:,:,10]) # Mask is generated from an atlas, atlas disregards certain subcortical regions that are just \"grey matter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bca5bb407d7483d9efc9b8f68b820bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indices = masked_data[1,:,:,:].nonzero()\n",
    "df_index = pd.MultiIndex.from_tuples(list(zip(*indices)), names=['row', 'col', 'depth'])\n",
    "series_df = pd.Series(index=df_index, dtype=object)\n",
    "\n",
    "for i, triple in tqdm_notebook(enumerate(zip(*indices)), total=len(indices[0])):\n",
    "    x, y, z = triple\n",
    "    trace = all_data[:, x, y, z]\n",
    "    \n",
    "    series_df.loc[triple] = trace\n",
    "\n",
    "data_block = np.vstack(series_df.values)[..., np.newaxis]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import quniform\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import math\n",
    "\n",
    "def data():\n",
    "    realigned_atlas = nib.load('../Data/big_atlas/coregistered_pig_2_atlas.nii').get_data()\n",
    "    mean_fmri = nib.load('../Data/pig_2/rfmri nii/meanus822-0016-00001-000001-01.nii').get_data()  \n",
    "\n",
    "    mask_roi_1 = np.ma.masked_where(np.isin(realigned_atlas, [0]), realigned_atlas)\n",
    "    fmri_masked_roi = np.ma.masked_where(np.ma.getmask(mask_roi_1), mean_fmri)\n",
    "    \n",
    "    im = mean_fmri\n",
    "    all_data = np.zeros((300, im.shape[0], im.shape[1], im.shape[2]))\n",
    "    masked_data = np.ma.zeros((300, im.shape[0], im.shape[1], im.shape[2]))\n",
    "\n",
    "    dicom_files = [x for x in Path('../Data/pig_2/rfmri nii/').iterdir() if x.is_file() and not 'ds_store' in str(x).lower()]\n",
    "    dicom_files.sort(key=str)\n",
    "    time_index = 0\n",
    "    mask_roi_1 = np.ma.masked_where(np.isin(realigned_atlas, [0]), realigned_atlas)\n",
    "\n",
    "    for f in dicom_files:\n",
    "        time_file = nib.load(str(f)).get_data()\n",
    "        fmri_masked_roi = np.ma.masked_where(np.ma.getmask(mask_roi_1), time_file) \n",
    "        try:\n",
    "            masked_data[time_index, ...] = fmri_masked_roi\n",
    "            all_data[time_index, ...] = time_file\n",
    "            time_index += 1\n",
    "        except:\n",
    "            print(f)\n",
    "    \n",
    "    \n",
    "    indices = masked_data[1,:,:,:].nonzero()\n",
    "    df_index = pd.MultiIndex.from_tuples(list(zip(*indices)), names=['row', 'col', 'depth'])\n",
    "    series_df = pd.Series(index=df_index, dtype=object)\n",
    "\n",
    "    for i, triple in enumerate(zip(*indices)):\n",
    "        x, y, z = triple\n",
    "        trace = all_data[:, x, y, z]\n",
    "\n",
    "        series_df.loc[triple] = trace\n",
    "    data_block = np.vstack(series_df.values)[..., np.newaxis]\n",
    "    \n",
    "    shuffled_data = np.copy(data_block)\n",
    "    np.random.shuffle(shuffled_data)\n",
    "\n",
    "    testing_set = shuffled_data[:700]  #hold back 700 for validation\n",
    "    training_set = shuffled_data[700:]\n",
    "    \n",
    "    x_train = training_set\n",
    "    y_train = training_set\n",
    "    \n",
    "    x_test = testing_set\n",
    "    y_test = testing_set\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "    \n",
    "    \n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    from keras.layers.convolutional import Conv1D, UpSampling1D\n",
    "    from keras.layers.pooling import MaxPooling1D\n",
    "    from keras.layers.core import Dense, Activation\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    c1_size = int({{quniform(50, 70, 1)}})\n",
    "    c1_length = int({{quniform(3, 15, 2)}})\n",
    "    \n",
    "    c2_size = int({{quniform(20, 40, 1)}})\n",
    "    c2_length = int({{quniform(3, 15, 2)}})\n",
    "    \n",
    "    c3_size = int({{quniform(10, 30, 1)}})\n",
    "    c3_length = int({{quniform(3, 15, 2)}})\n",
    "    \n",
    "    c4_size = int({{quniform(1, 8, 1)}})\n",
    "    c4_length = int({{quniform(3, 15, 2)}})\n",
    "      \n",
    "    # encoder\n",
    "    model.add(Conv1D(c1_size, c1_length, \n",
    "                     padding='causal', input_shape=[300, 1])) #Dimensionality 300\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv1D(c2_size, c2_length, \n",
    "                     padding='causal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    \n",
    "    model.add(Conv1D(c3_size, c3_length, \n",
    "                     padding='causal')) # Activation map size [150 x 1]\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    \n",
    "    model.add(Conv1D(c4_size, c4_length, \n",
    "                     padding='causal')) # Activation map size [75 x 1]\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    \n",
    "    # Activation map size [25 x 1] \n",
    "    # Encoding represenation size is [25 x 1 x N (8)] = 200\n",
    "    # Dimensionality reduction of 300 -> 200\n",
    "    \n",
    "    # decoder\n",
    "    model.add(Conv1D(c4_size, c4_length, \n",
    "                     padding='causal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(UpSampling1D(size=3))\n",
    "    \n",
    "    model.add(Conv1D(c3_size, c3_length, \n",
    "                     padding='causal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(UpSampling1D())\n",
    "    \n",
    "    model.add(Conv1D(c2_size, c2_length, \n",
    "                     padding='causal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(UpSampling1D())\n",
    "    \n",
    "    model.add(Conv1D(c1_size, c1_length, \n",
    "                     padding='causal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv1D(1, 1, padding='causal'))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    # section 2.1 mentions a special regularization, but I don't think that is going to work\n",
    "    # the feature map in the compressed representation and the output will not be timeshuffled if we use causal conv\n",
    "    \n",
    "    # learning rate schedule\n",
    "    def step_decay(epoch):\n",
    "        initial_lrate = 0.01\n",
    "        drop = 0.5\n",
    "        epochs_drop = 10.0\n",
    "        lrate = initial_lrate * drop**(np.floor((1+epoch)/epochs_drop))\n",
    "        return lrate\n",
    "    \n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    callbacks_list = [lrate, keras.callbacks.TerminateOnNaN()]\n",
    "    \n",
    "    model.compile(optimizer={{choice(['rmsprop', 'adam', 'sgd'])}}, loss='mse')\n",
    "    model.fit(x_train, y_train, epochs=4, batch_size={{choice([32, 64, 128, 256])}}, \n",
    "              callbacks=callbacks_list, validation_data=(x_test, x_test), verbose=2)\n",
    "    \n",
    "    loss = model.evaluate(x_test, x_test)\n",
    "    \n",
    "    if np.isnan(loss):\n",
    "        loss = float('inf')\n",
    "        \n",
    "    \n",
    "    return {'loss': loss, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import dicom\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from pathlib import Path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tqdm import tqdm_notebook\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import scipy.misc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from skimage import io\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import scipy.stats\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import skimage.color\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from skimage.filters import threshold_otsu\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import animation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from IPython.display import HTML\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from PIL import Image\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import nibabel as nib\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nibabel.testing import data_path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import dill\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import quniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import LearningRateScheduler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import math\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import Conv1D, UpSampling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.pooling import MaxPooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.normalization import BatchNormalization\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt.mongoexp import MongoTrials\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'int': hp.quniform('int', 50, 70, 1),\n",
      "        'int_1': hp.quniform('int_1', 3, 15, 2),\n",
      "        'int_2': hp.quniform('int_2', 20, 40, 1),\n",
      "        'int_3': hp.quniform('int_3', 3, 15, 2),\n",
      "        'int_4': hp.quniform('int_4', 10, 30, 1),\n",
      "        'int_5': hp.quniform('int_5', 3, 15, 2),\n",
      "        'int_6': hp.quniform('int_6', 1, 8, 1),\n",
      "        'int_7': hp.quniform('int_7', 3, 15, 2),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "        'batch_size': hp.choice('batch_size', [32, 64, 128, 256]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: realigned_atlas = nib.load('../Data/big_atlas/coregistered_pig_2_atlas.nii').get_data()\n",
      "   3: mean_fmri = nib.load('../Data/pig_2/rfmri nii/meanus822-0016-00001-000001-01.nii').get_data()  \n",
      "   4: \n",
      "   5: mask_roi_1 = np.ma.masked_where(np.isin(realigned_atlas, [0]), realigned_atlas)\n",
      "   6: fmri_masked_roi = np.ma.masked_where(np.ma.getmask(mask_roi_1), mean_fmri)\n",
      "   7: \n",
      "   8: im = mean_fmri\n",
      "   9: all_data = np.zeros((300, im.shape[0], im.shape[1], im.shape[2]))\n",
      "  10: masked_data = np.ma.zeros((300, im.shape[0], im.shape[1], im.shape[2]))\n",
      "  11: \n",
      "  12: dicom_files = [x for x in Path('../Data/pig_2/rfmri nii/').iterdir() if x.is_file() and not 'ds_store' in str(x).lower()]\n",
      "  13: dicom_files.sort(key=str)\n",
      "  14: time_index = 0\n",
      "  15: mask_roi_1 = np.ma.masked_where(np.isin(realigned_atlas, [0]), realigned_atlas)\n",
      "  16: \n",
      "  17: for f in dicom_files:\n",
      "  18:     time_file = nib.load(str(f)).get_data()\n",
      "  19:     fmri_masked_roi = np.ma.masked_where(np.ma.getmask(mask_roi_1), time_file) \n",
      "  20:     try:\n",
      "  21:         masked_data[time_index, ...] = fmri_masked_roi\n",
      "  22:         all_data[time_index, ...] = time_file\n",
      "  23:         time_index += 1\n",
      "  24:     except:\n",
      "  25:         print(f)\n",
      "  26: \n",
      "  27: \n",
      "  28: indices = masked_data[1,:,:,:].nonzero()\n",
      "  29: df_index = pd.MultiIndex.from_tuples(list(zip(*indices)), names=['row', 'col', 'depth'])\n",
      "  30: series_df = pd.Series(index=df_index, dtype=object)\n",
      "  31: \n",
      "  32: for i, triple in enumerate(zip(*indices)):\n",
      "  33:     x, y, z = triple\n",
      "  34:     trace = all_data[:, x, y, z]\n",
      "  35: \n",
      "  36:     series_df.loc[triple] = trace\n",
      "  37: data_block = np.vstack(series_df.values)[..., np.newaxis]\n",
      "  38: \n",
      "  39: shuffled_data = np.copy(data_block)\n",
      "  40: np.random.shuffle(shuffled_data)\n",
      "  41: \n",
      "  42: testing_set = shuffled_data[:700]  #hold back 700 for validation\n",
      "  43: training_set = shuffled_data[700:]\n",
      "  44: \n",
      "  45: x_train = training_set\n",
      "  46: y_train = training_set\n",
      "  47: \n",
      "  48: x_test = testing_set\n",
      "  49: y_test = testing_set\n",
      "  50: \n",
      "  51: \n",
      "  52: \n",
      "  53: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     model = keras.models.Sequential()\n",
      "   5:     \n",
      "   6:     c1_size = int(space['int'])\n",
      "   7:     c1_length = int(space['int_1'])\n",
      "   8:     \n",
      "   9:     c2_size = int(space['int_2'])\n",
      "  10:     c2_length = int(space['int_3'])\n",
      "  11:     \n",
      "  12:     c3_size = int(space['int_4'])\n",
      "  13:     c3_length = int(space['int_5'])\n",
      "  14:     \n",
      "  15:     c4_size = int(space['int_6'])\n",
      "  16:     c4_length = int(space['int_7'])\n",
      "  17:       \n",
      "  18:     # encoder\n",
      "  19:     model.add(Conv1D(c1_size, c1_length, \n",
      "  20:                      padding='causal', input_shape=[300, 1])) #Dimensionality 300\n",
      "  21:     model.add(BatchNormalization())\n",
      "  22:     model.add(Activation('relu'))\n",
      "  23: \n",
      "  24:     model.add(Conv1D(c2_size, c2_length, \n",
      "  25:                      padding='causal'))\n",
      "  26:     model.add(BatchNormalization())\n",
      "  27:     model.add(Activation('relu'))\n",
      "  28:     model.add(MaxPooling1D())\n",
      "  29:     \n",
      "  30:     model.add(Conv1D(c3_size, c3_length, \n",
      "  31:                      padding='causal')) # Activation map size [150 x 1]\n",
      "  32:     model.add(BatchNormalization())\n",
      "  33:     model.add(Activation('relu'))\n",
      "  34:     model.add(MaxPooling1D())\n",
      "  35:     \n",
      "  36:     model.add(Conv1D(c4_size, c4_length, \n",
      "  37:                      padding='causal')) # Activation map size [75 x 1]\n",
      "  38:     model.add(BatchNormalization())\n",
      "  39:     model.add(Activation('relu'))\n",
      "  40:     model.add(MaxPooling1D(pool_size=3))\n",
      "  41:     \n",
      "  42:     # Activation map size [25 x 1] \n",
      "  43:     # Encoding represenation size is [25 x 1 x N (8)] = 200\n",
      "  44:     # Dimensionality reduction of 300 -> 200\n",
      "  45:     \n",
      "  46:     # decoder\n",
      "  47:     model.add(Conv1D(c4_size, c4_length, \n",
      "  48:                      padding='causal'))\n",
      "  49:     model.add(BatchNormalization())\n",
      "  50:     model.add(Activation('relu'))\n",
      "  51:     model.add(UpSampling1D(size=3))\n",
      "  52:     \n",
      "  53:     model.add(Conv1D(c3_size, c3_length, \n",
      "  54:                      padding='causal'))\n",
      "  55:     model.add(BatchNormalization())\n",
      "  56:     model.add(Activation('relu'))\n",
      "  57:     model.add(UpSampling1D())\n",
      "  58:     \n",
      "  59:     model.add(Conv1D(c2_size, c2_length, \n",
      "  60:                      padding='causal'))\n",
      "  61:     model.add(BatchNormalization())\n",
      "  62:     model.add(Activation('relu'))\n",
      "  63:     model.add(UpSampling1D())\n",
      "  64:     \n",
      "  65:     model.add(Conv1D(c1_size, c1_length, \n",
      "  66:                      padding='causal'))\n",
      "  67:     model.add(BatchNormalization())\n",
      "  68:     model.add(Activation('relu'))\n",
      "  69:     \n",
      "  70:     model.add(Conv1D(1, 1, padding='causal'))\n",
      "  71:     model.add(Activation('linear'))\n",
      "  72:     \n",
      "  73:     # section 2.1 mentions a special regularization, but I don't think that is going to work\n",
      "  74:     # the feature map in the compressed representation and the output will not be timeshuffled if we use causal conv\n",
      "  75:     \n",
      "  76:     # learning rate schedule\n",
      "  77:     def step_decay(epoch):\n",
      "  78:         initial_lrate = 0.01\n",
      "  79:         drop = 0.5\n",
      "  80:         epochs_drop = 10.0\n",
      "  81:         lrate = initial_lrate * drop**(np.floor((1+epoch)/epochs_drop))\n",
      "  82:         return lrate\n",
      "  83:     \n",
      "  84:     lrate = LearningRateScheduler(step_decay)\n",
      "  85:     callbacks_list = [lrate, keras.callbacks.TerminateOnNaN()]\n",
      "  86:     \n",
      "  87:     model.compile(optimizer=space['optimizer'], loss='mse')\n",
      "  88:     model.fit(x_train, y_train, epochs=4, batch_size=space['batch_size'], \n",
      "  89:               callbacks=callbacks_list, validation_data=(x_test, x_test), verbose=2)\n",
      "  90:     \n",
      "  91:     loss = model.evaluate(x_test, x_test)\n",
      "  92:     \n",
      "  93:     if np.isnan(loss):\n",
      "  94:         loss = float('inf')\n",
      "  95:         \n",
      "  96:     \n",
      "  97:     return {'loss': loss, 'status': STATUS_OK, 'model': model}\n",
      "  98: \n",
      "../Data/pig_2/rfmri nii/meanus822-0016-00001-000001-01.nii\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 3356706.2145 - val_loss: 1526904.6400\n",
      "Epoch 2/4\n",
      "12s - loss: 3257246.0615 - val_loss: 2121787.2671\n",
      "Epoch 3/4\n",
      "12s - loss: 3132801.5151 - val_loss: 1473245.0371\n",
      "Epoch 4/4\n",
      "11s - loss: 2978407.9351 - val_loss: 1770156.2657\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "20s - loss: 3171468.7251 - val_loss: 1381114.3629\n",
      "Epoch 2/4\n",
      "19s - loss: 2033315.1925 - val_loss: 8312032.0971\n",
      "Epoch 3/4\n",
      "19s - loss: 731120.1392 - val_loss: 4198831.1086\n",
      "Epoch 4/4\n",
      "19s - loss: 160295.3274 - val_loss: 856713.6504\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "14s - loss: 3214746.4188 - val_loss: 9985950.9086\n",
      "Epoch 2/4\n",
      "14s - loss: 2197943.3811 - val_loss: 4314161.2514\n",
      "Epoch 3/4\n",
      "14s - loss: 895986.1409 - val_loss: 83934.8558\n",
      "Epoch 4/4\n",
      "14s - loss: 204830.3384 - val_loss: 33821.7818\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "19s - loss: 3290497.3856 - val_loss: 949708.6425\n",
      "Epoch 2/4\n",
      "18s - loss: 2976086.5646 - val_loss: 675959.5196\n",
      "Epoch 3/4\n",
      "18s - loss: 2537454.0719 - val_loss: 779340.2875\n",
      "Epoch 4/4\n",
      "18s - loss: 2019723.2574 - val_loss: 194672.6506\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "14s - loss: 3359772.8127 - val_loss: 2925262.4629\n",
      "Epoch 2/4\n",
      "14s - loss: 3267396.1297 - val_loss: 907901.6946\n",
      "Epoch 3/4\n",
      "14s - loss: 3141897.1136 - val_loss: 312289.4441\n",
      "Epoch 4/4\n",
      "14s - loss: 2991423.9723 - val_loss: 1746879.4014\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 3288291.8964 - val_loss: 331342.4727\n",
      "Epoch 2/4\n",
      "15s - loss: 2992255.5770 - val_loss: 1524760.7871\n",
      "Epoch 3/4\n",
      "15s - loss: 2575039.8374 - val_loss: 92770.9241\n",
      "Epoch 4/4\n",
      "15s - loss: 2075192.9701 - val_loss: 239155.8108\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "15s - loss: 3335018.2311 - val_loss: 1698761.6407\n",
      "Epoch 2/4\n",
      "15s - loss: 3044358.6157 - val_loss: 1118581.9850\n",
      "Epoch 3/4\n",
      "15s - loss: 2449420.5489 - val_loss: 367956.2075\n",
      "Epoch 4/4\n",
      "15s - loss: 1668159.4241 - val_loss: 273099.5760\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "15s - loss: 2621792.4241 - val_loss: 2341266.3343\n",
      "Epoch 2/4\n",
      "14s - loss: 471706.1286 - val_loss: 1569358.0729\n",
      "Epoch 3/4\n",
      "14s - loss: 51616.2501 - val_loss: 7161663.6486\n",
      "Epoch 4/4\n",
      "14s - loss: 25322.4143 - val_loss: 89821.2342\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "11s - loss: 3340776.5574 - val_loss: 2494774.0286\n",
      "Epoch 2/4\n",
      "10s - loss: 3071923.7355 - val_loss: 1566695.8379\n",
      "Epoch 3/4\n",
      "10s - loss: 2544648.8960 - val_loss: 193374.2400\n",
      "Epoch 4/4\n",
      "10s - loss: 1859457.5507 - val_loss: 41109.8450\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "14s - loss: 3371596.3119 - val_loss: 1422483.6171\n",
      "Epoch 2/4\n",
      "14s - loss: 3298124.9242 - val_loss: 7843186.5143\n",
      "Epoch 3/4\n",
      "14s - loss: 3144374.5755 - val_loss: 2623835.2743\n",
      "Epoch 4/4\n",
      "14s - loss: 2914183.4215 - val_loss: 3333608.0714\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "14s - loss: 3283979.2862 - val_loss: 2606550.0514\n",
      "Epoch 2/4\n",
      "13s - loss: 2974928.0423 - val_loss: 843844.5871\n",
      "Epoch 3/4\n",
      "14s - loss: 2541150.3881 - val_loss: 497804.1934\n",
      "Epoch 4/4\n",
      "14s - loss: 2025227.9844 - val_loss: 135315.6707\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "14s - loss: 3222252.0089 - val_loss: 212863.3792\n",
      "Epoch 2/4\n",
      "13s - loss: 2225835.7080 - val_loss: 9623145.7143\n",
      "Epoch 3/4\n",
      "13s - loss: 945328.4937 - val_loss: 615548.1161\n",
      "Epoch 4/4\n",
      "13s - loss: 247382.0793 - val_loss: 710523.7211\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "11s - loss: 3342672.5052 - val_loss: 1338972.5007\n",
      "Epoch 2/4\n",
      "10s - loss: 3072826.4541 - val_loss: 2757356.7114\n",
      "Epoch 3/4\n",
      "10s - loss: 2542586.3692 - val_loss: 269666.7029\n",
      "Epoch 4/4\n",
      "10s - loss: 1857576.8647 - val_loss: 388361.7968\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 2632533.8744 - val_loss: 413972.1157\n",
      "Epoch 2/4\n",
      "15s - loss: 491327.6545 - val_loss: 4080711.8157\n",
      "Epoch 3/4\n",
      "15s - loss: 59008.6973 - val_loss: 481428.2909\n",
      "Epoch 4/4\n",
      "15s - loss: 34867.9675 - val_loss: 568302.8804\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "11s - loss: 3357093.9142 - val_loss: 4641624.6257\n",
      "Epoch 2/4\n",
      "10s - loss: 3258207.4192 - val_loss: 1404566.9129\n",
      "Epoch 3/4\n",
      "11s - loss: 3112587.7489 - val_loss: 324311.4570\n",
      "Epoch 4/4\n",
      "11s - loss: 2943337.4703 - val_loss: 134278.2115\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 3297072.5271 - val_loss: 1632378.5193\n",
      "Epoch 2/4\n",
      "13s - loss: 3001560.6924 - val_loss: 2485316.3071\n",
      "Epoch 3/4\n",
      "13s - loss: 2590793.6561 - val_loss: 100122.8171\n",
      "Epoch 4/4\n",
      "13s - loss: 2098141.3882 - val_loss: 177047.2991\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "18s - loss: 2726748.3457 - val_loss: 955124.3229\n",
      "Epoch 2/4\n",
      "17s - loss: 589976.2544 - val_loss: 2354870.1871\n",
      "Epoch 3/4\n",
      "17s - loss: 87767.3101 - val_loss: 332533.9925\n",
      "Epoch 4/4\n",
      "17s - loss: 44729.9809 - val_loss: 203397.8765\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "15s - loss: 3194969.1325 - val_loss: 1285666.7736\n",
      "Epoch 2/4\n",
      "14s - loss: 2150043.8886 - val_loss: 600579.2536\n",
      "Epoch 3/4\n",
      "14s - loss: 865589.8312 - val_loss: 2699475.1257\n",
      "Epoch 4/4\n",
      "14s - loss: 216554.1905 - val_loss: 795654.3789\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 3214391.3440 - val_loss: 1201412.8150\n",
      "Epoch 2/4\n",
      "12s - loss: 2254199.4327 - val_loss: 418400.5912\n",
      "Epoch 3/4\n",
      "12s - loss: 993822.5513 - val_loss: 1045053.7961\n",
      "Epoch 4/4\n",
      "12s - loss: 277467.9521 - val_loss: 3822350.1986\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 3203276.3449 - val_loss: 9132604.0057\n",
      "Epoch 2/4\n",
      "15s - loss: 2165892.7200 - val_loss: 159776.8172\n",
      "Epoch 3/4\n",
      "15s - loss: 874796.8665 - val_loss: 268391.2305\n",
      "Epoch 4/4\n",
      "15s - loss: 212897.8235 - val_loss: 615484.3082\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 3226156.6661 - val_loss: 2063009.2650\n",
      "Epoch 2/4\n",
      "12s - loss: 2288531.9174 - val_loss: 3507430.4271\n",
      "Epoch 3/4\n",
      "12s - loss: 1004821.1526 - val_loss: 481132.0511\n",
      "Epoch 4/4\n",
      "13s - loss: 224843.8569 - val_loss: 1157200.3386\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "14s - loss: 3193349.3262 - val_loss: 123406.6354\n",
      "Epoch 2/4\n",
      "13s - loss: 2136499.9363 - val_loss: 6733116.7629\n",
      "Epoch 3/4\n",
      "13s - loss: 840880.5285 - val_loss: 520680.6255\n",
      "Epoch 4/4\n",
      "13s - loss: 198788.8460 - val_loss: 575886.5757\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 3223712.0716 - val_loss: 2231338.9757\n",
      "Epoch 2/4\n",
      "12s - loss: 2284687.6564 - val_loss: 5178889.1086\n",
      "Epoch 3/4\n",
      "12s - loss: 1039934.2301 - val_loss: 3892693.4029\n",
      "Epoch 4/4\n",
      "12s - loss: 301983.2474 - val_loss: 702242.9021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 3346230.8887 - val_loss: 3237025.4043\n",
      "Epoch 2/4\n",
      "11s - loss: 3091296.1851 - val_loss: 1054230.6357\n",
      "Epoch 3/4\n",
      "11s - loss: 2555714.2621 - val_loss: 151333.2580\n",
      "Epoch 4/4\n",
      "11s - loss: 1875465.8949 - val_loss: 262911.0925\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 3373825.5829 - val_loss: 2073506.0493\n",
      "Epoch 2/4\n",
      "11s - loss: 3311148.7554 - val_loss: 1175596.6536\n",
      "Epoch 3/4\n",
      "12s - loss: 3172412.7361 - val_loss: 172724.6818\n",
      "Epoch 4/4\n",
      "12s - loss: 2965606.3600 - val_loss: 417383.7575\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 3209611.2492 - val_loss: 2370513.2329\n",
      "Epoch 2/4\n",
      "11s - loss: 2190536.9024 - val_loss: 1009875.8207\n",
      "Epoch 3/4\n",
      "11s - loss: 915379.0598 - val_loss: 618541.1229\n",
      "Epoch 4/4\n",
      "10s - loss: 235578.8340 - val_loss: 418874.7221\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "14s - loss: 3343988.8924 - val_loss: 1311101.0636\n",
      "Epoch 2/4\n",
      "13s - loss: 3089899.9448 - val_loss: 5978245.1057\n",
      "Epoch 3/4\n",
      "13s - loss: 2587282.0455 - val_loss: 1204108.8664\n",
      "Epoch 4/4\n",
      "13s - loss: 1933299.9939 - val_loss: 52325.2386\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 3371339.6565 - val_loss: 2802959.6571\n",
      "Epoch 2/4\n",
      "12s - loss: 3302980.0741 - val_loss: 3236816.0700\n",
      "Epoch 3/4\n",
      "12s - loss: 3166956.5924 - val_loss: 2212209.6200\n",
      "Epoch 4/4\n",
      "12s - loss: 2947565.8966 - val_loss: 540819.4107\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 3315503.0454 - val_loss: 3430479.6557\n",
      "Epoch 2/4\n",
      "11s - loss: 3067047.8558 - val_loss: 3270099.3600\n",
      "Epoch 3/4\n",
      "11s - loss: 2720186.0435 - val_loss: 2371795.7400\n",
      "Epoch 4/4\n",
      "11s - loss: 2295005.6402 - val_loss: 1449584.9243\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 3194100.0388 - val_loss: 13139285.6057\n",
      "Epoch 2/4\n",
      "15s - loss: 2119541.5797 - val_loss: 615149.7150\n",
      "Epoch 3/4\n",
      "15s - loss: 820692.0213 - val_loss: 10136981.2171\n",
      "Epoch 4/4\n",
      "15s - loss: 178117.2567 - val_loss: 3079322.4914\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 2696204.7129 - val_loss: 2011724.5079\n",
      "Epoch 2/4\n",
      "14s - loss: 566733.6819 - val_loss: 1738385.1071\n",
      "Epoch 3/4\n",
      "14s - loss: 61729.8385 - val_loss: 750500.0082\n",
      "Epoch 4/4\n",
      "14s - loss: 26679.6012 - val_loss: 182653.3267\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "14s - loss: 3351819.7594 - val_loss: 3422588.7186\n",
      "Epoch 2/4\n",
      "12s - loss: 3250745.9925 - val_loss: 2759909.5543\n",
      "Epoch 3/4\n",
      "12s - loss: 3085214.0122 - val_loss: 1765820.6107\n",
      "Epoch 4/4\n",
      "12s - loss: 2902150.4531 - val_loss: 1549381.4800\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "17s - loss: 3346159.9082 - val_loss: 1048370.1829\n",
      "Epoch 2/4\n",
      "15s - loss: 3089808.4767 - val_loss: 900699.9307\n",
      "Epoch 3/4\n",
      "15s - loss: 2576688.1546 - val_loss: 651394.4318\n",
      "Epoch 4/4\n",
      "15s - loss: 1910507.4643 - val_loss: 105629.1104\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "700/700 [==============================] - 1s     \n",
      "Train on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 3163480.1928 - val_loss: 1271644.9257\n",
      "Epoch 2/4\n",
      "12s - loss: 1980684.4404 - val_loss: 3054908.3171\n",
      "Epoch 3/4\n",
      "12s - loss: 672032.0670 - val_loss: 4810509.1914\n",
      "Epoch 4/4\n",
      "12s - loss: 141193.4979 - val_loss: 2724702.8471\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "19s - loss: 2818867.9448 - val_loss: 372895.0348\n",
      "Epoch 2/4\n",
      "17s - loss: 1068214.5451 - val_loss: 238393.9338\n",
      "Epoch 3/4\n",
      "17s - loss: 102111.6825 - val_loss: 763202.8171\n",
      "Epoch 4/4\n",
      "17s - loss: 45471.6449 - val_loss: 521479.8023\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "15s - loss: 3338892.3249 - val_loss: 3199367.4914\n",
      "Epoch 2/4\n",
      "13s - loss: 3051749.7546 - val_loss: 2400393.2343\n",
      "Epoch 3/4\n",
      "13s - loss: 2417266.6502 - val_loss: 792411.6571\n",
      "Epoch 4/4\n",
      "13s - loss: 1616336.8384 - val_loss: 405963.8629\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 3375199.8359 - val_loss: 2507390.7414\n",
      "Epoch 2/4\n",
      "12s - loss: 3310547.0817 - val_loss: 1337099.0257\n",
      "Epoch 3/4\n",
      "12s - loss: 3168265.7851 - val_loss: 951537.9614\n",
      "Epoch 4/4\n",
      "12s - loss: 2954988.0857 - val_loss: 244883.7810\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "15s - loss: 3299323.1324 - val_loss: 1373606.1950\n",
      "Epoch 2/4\n",
      "13s - loss: 3029809.3736 - val_loss: 1111347.8350\n",
      "Epoch 3/4\n",
      "13s - loss: 2649295.8110 - val_loss: 112832.0200\n",
      "Epoch 4/4\n",
      "14s - loss: 2184963.8568 - val_loss: 364819.6948\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "15s - loss: 3214427.9954 - val_loss: 2013882.0914\n",
      "Epoch 2/4\n",
      "13s - loss: 2240362.4045 - val_loss: 1699022.7386\n",
      "Epoch 3/4\n",
      "12s - loss: 977640.6001 - val_loss: 1509856.8714\n",
      "Epoch 4/4\n",
      "12s - loss: 278173.0459 - val_loss: 1089991.0479\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 4: Invalid loss, terminating training\n",
      "700/700 [==============================] - 1s     \n",
      "Train on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "14s - loss: 3370829.0796 - val_loss: 3026330.2657\n",
      "Epoch 2/4\n",
      "12s - loss: 3287714.9701 - val_loss: 2566542.1886\n",
      "Epoch 3/4\n",
      "12s - loss: 3132264.2873 - val_loss: 2358411.8729\n",
      "Epoch 4/4\n",
      "12s - loss: 2896494.4409 - val_loss: 2533291.4586\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 3305391.7942 - val_loss: 449060.9437\n",
      "Epoch 2/4\n",
      "14s - loss: 3039536.4031 - val_loss: 293374.4345\n",
      "Epoch 3/4\n",
      "14s - loss: 2668599.8150 - val_loss: 1253340.3250\n",
      "Epoch 4/4\n",
      "14s - loss: 2217769.0031 - val_loss: 521233.0693\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 3217536.0927 - val_loss: 745843.0532\n",
      "Epoch 2/4\n",
      "11s - loss: 2248916.5033 - val_loss: 4287875.9000\n",
      "Epoch 3/4\n",
      "12s - loss: 966535.1808 - val_loss: 2530668.5786\n",
      "Epoch 4/4\n",
      "11s - loss: 247723.9864 - val_loss: 7516029.4486\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "15s - loss: 3339534.1505 - val_loss: 1998679.2521\n",
      "Epoch 2/4\n",
      "13s - loss: 3057794.5625 - val_loss: 887418.5389\n",
      "Epoch 3/4\n",
      "13s - loss: 2507249.3550 - val_loss: 215618.7892\n",
      "Epoch 4/4\n",
      "13s - loss: 1802717.0549 - val_loss: 198102.7289\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "17s - loss: 3212720.3470 - val_loss: 16776767.7714\n",
      "Epoch 2/4\n",
      "15s - loss: 2229743.2540 - val_loss: 232117603.3829\n",
      "Epoch 3/4\n",
      "15s - loss: 954409.4932 - val_loss: 2068809.8029\n",
      "Epoch 4/4\n",
      "15s - loss: 240549.6237 - val_loss: 1406858.8543\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15s - loss: 3168840.3429 - val_loss: 2875183.6086\n",
      "Epoch 2/4\n",
      "13s - loss: 2431255.1583 - val_loss: 1747704.8129\n",
      "Epoch 3/4\n",
      "14s - loss: 1458984.7005 - val_loss: 661642.1739\n",
      "Epoch 4/4\n",
      "14s - loss: 563391.8797 - val_loss: 376971.5020\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 3335655.0179 - val_loss: 3084930.6729\n",
      "Epoch 2/4\n",
      "11s - loss: 3050510.5366 - val_loss: 1987980.0736\n",
      "Epoch 3/4\n",
      "11s - loss: 2398050.1065 - val_loss: 1611629.0536\n",
      "Epoch 4/4\n",
      "11s - loss: 1591466.9399 - val_loss: 136612.3683\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 3375644.7647 - val_loss: 1635196.4186\n",
      "Epoch 2/4\n",
      "14s - loss: 3314278.0677 - val_loss: 1750288.3793\n",
      "Epoch 3/4\n",
      "13s - loss: 3191116.8808 - val_loss: 648626.2461\n",
      "Epoch 4/4\n",
      "14s - loss: 2995854.8965 - val_loss: 4661492.1771\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "15s - loss: 3214222.9125 - val_loss: 2010718.1143\n",
      "Epoch 2/4\n",
      "13s - loss: 2232155.5956 - val_loss: 863224.4675\n",
      "Epoch 3/4\n",
      "13s - loss: 763191.7662 - val_loss: 3320442.4543\n",
      "Epoch 4/4\n",
      "13s - loss: 126770.8591 - val_loss: 1245885.8150\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "700/700 [==============================] - 1s     \n",
      "Train on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 2598975.7430 - val_loss: 2009477.3493\n",
      "Epoch 2/4\n",
      "14s - loss: 456621.6035 - val_loss: 12790039.5429\n",
      "Epoch 3/4\n",
      "14s - loss: 67073.5849 - val_loss: 897454.8064\n",
      "Epoch 4/4\n",
      "14s - loss: 35689.5828 - val_loss: 337563.2868\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "18s - loss: 3168417.7482 - val_loss: 374291.5375\n",
      "Epoch 2/4\n",
      "15s - loss: 2429933.1522 - val_loss: 982428.2793\n",
      "Epoch 3/4\n",
      "15s - loss: 1454765.9570 - val_loss: 234376.7782\n",
      "Epoch 4/4\n",
      "15s - loss: 577271.1044 - val_loss: 138813.5654\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 3339059.6957 - val_loss: 2990168.8614\n",
      "Epoch 2/4\n",
      "14s - loss: 3062681.5574 - val_loss: 1577680.3100\n",
      "Epoch 3/4\n",
      "14s - loss: 2492574.1829 - val_loss: 1087150.6807\n",
      "Epoch 4/4\n",
      "14s - loss: 1769132.3810 - val_loss: 904100.1768\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 3375207.8848 - val_loss: 3446395.2571\n",
      "Epoch 2/4\n",
      "10s - loss: 3311996.4473 - val_loss: 3289112.3400\n",
      "Epoch 3/4\n",
      "10s - loss: 3179268.3094 - val_loss: 2947070.7229\n",
      "Epoch 4/4\n",
      "10s - loss: 2970436.0622 - val_loss: 2703381.4457\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "Batch 2: Invalid loss, terminating training\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 3344560.4689 - val_loss: 3417343.7229\n",
      "Epoch 2/4\n",
      "14s - loss: 3088443.4823 - val_loss: 1795215.8736\n",
      "Epoch 3/4\n",
      "14s - loss: 2579373.0588 - val_loss: 741690.0593\n",
      "Epoch 4/4\n",
      "14s - loss: 1917569.0267 - val_loss: 103173.6199\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "17s - loss: 3344186.2102 - val_loss: 79488184.0914\n",
      "Epoch 2/4\n",
      "15s - loss: 3086627.1026 - val_loss: 8011948.7429\n",
      "Epoch 3/4\n",
      "15s - loss: 2570457.6272 - val_loss: 287598.4684\n",
      "Epoch 4/4\n",
      "15s - loss: 1899610.8415 - val_loss: 223219.3165\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 3345507.4413 - val_loss: 390975.9180\n",
      "Epoch 2/4\n",
      "14s - loss: 3094512.8630 - val_loss: 315462.8989\n",
      "Epoch 3/4\n",
      "14s - loss: 2597334.7998 - val_loss: 470575.4623\n",
      "Epoch 4/4\n",
      "14s - loss: 1943445.3750 - val_loss: 355371.3548\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "17s - loss: 3343687.0735 - val_loss: 47670052.8457\n",
      "Epoch 2/4\n",
      "14s - loss: 3093837.7887 - val_loss: 312848.0868\n",
      "Epoch 3/4\n",
      "15s - loss: 2598792.8128 - val_loss: 3293587.5100\n",
      "Epoch 4/4\n",
      "15s - loss: 1946986.7348 - val_loss: 1372187.3207\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "17s - loss: 3342413.6874 - val_loss: 841992.1929\n",
      "Epoch 2/4\n",
      "14s - loss: 3076806.0831 - val_loss: 1452081.5479\n",
      "Epoch 3/4\n",
      "14s - loss: 2551059.7368 - val_loss: 2045268.1914\n",
      "Epoch 4/4\n",
      "14s - loss: 1869552.5944 - val_loss: 195868.7197\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "18s - loss: 3339405.5606 - val_loss: 1449014.4271\n",
      "Epoch 2/4\n",
      "16s - loss: 3063339.2475 - val_loss: 440443.6859\n",
      "Epoch 3/4\n",
      "16s - loss: 2514780.1649 - val_loss: 125994.5046\n",
      "Epoch 4/4\n",
      "16s - loss: 1813524.3705 - val_loss: 202567.9384\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "19s - loss: 3338887.0048 - val_loss: 2004948.9229\n",
      "Epoch 2/4\n",
      "16s - loss: 3066780.2727 - val_loss: 412338.4437\n",
      "Epoch 3/4\n",
      "16s - loss: 2523422.5940 - val_loss: 2155614.4586\n",
      "Epoch 4/4\n",
      "16s - loss: 1826555.2979 - val_loss: 454955.8286\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "15s - loss: 3347904.3695 - val_loss: 2448580.9743\n",
      "Epoch 2/4\n",
      "12s - loss: 3093815.0325 - val_loss: 5794486.4886\n",
      "Epoch 3/4\n",
      "12s - loss: 2556697.6902 - val_loss: 445502.2123\n",
      "Epoch 4/4\n",
      "12s - loss: 1873563.7464 - val_loss: 955571.2568\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 3337128.3771 - val_loss: 3093801.8214\n",
      "Epoch 2/4\n",
      "14s - loss: 3045893.2978 - val_loss: 2253605.2243\n",
      "Epoch 3/4\n",
      "13s - loss: 2483409.7986 - val_loss: 598938.3468\n",
      "Epoch 4/4\n",
      "13s - loss: 1768603.1451 - val_loss: 146985.7268\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "18s - loss: 3208936.4459 - val_loss: 5477402.4686\n",
      "Epoch 2/4\n",
      "16s - loss: 2200716.1533 - val_loss: 12176387.4743\n",
      "Epoch 3/4\n",
      "16s - loss: 931680.2310 - val_loss: 157276.8908\n",
      "Epoch 4/4\n",
      "16s - loss: 250428.4922 - val_loss: 1033985.5479\n",
      "672/700 [===========================>..] - ETA: 0sTrain on 6455 samples, validate on 700 samples\n",
      "Epoch 1/4\n",
      "21s - loss: 2663515.4387 - val_loss: 8934492.0514\n",
      "Epoch 2/4\n",
      "18s - loss: 532030.8582 - val_loss: 1144485.4707\n",
      "Epoch 3/4\n",
      "19s - loss: 52680.2637 - val_loss: 156089.6895\n",
      "Epoch 4/4\n",
      "19s - loss: 44420.2068 - val_loss: 62069.4306\n",
      "672/700 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from hyperopt.mongoexp import MongoTrials\n",
    "\n",
    "# getting this thing to work with mongotrials is NonTrivial, but also NonImpossible\n",
    "# trials = MongoTrials('mongo://localhost:1234/foo_db/jobs', exp_key='exp1')\n",
    "\n",
    "best_run, best_model = optim.minimize(model=model, data=data, algo=tpe.suggest, \n",
    "                                      max_evals=100, trials=Trials(),\n",
    "                                     notebook_name='Experiment 11 - hyperopt', verbose=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
